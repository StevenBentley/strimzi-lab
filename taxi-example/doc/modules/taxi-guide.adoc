:path-main: src/main/java/io/strimzi
:url-gh-root: https://github.com/adam-cattermole/strimzi-lab/tree/add-taxi-example/taxi-example
:url-taxi-connect: {url-gh-root}/taxi-connect/{path-main}
:url-taxi-producer: {url-gh-root}/taxi-producer/{path-main}
:url-trip-convert: {url-gh-root}/trip-convert-app/{path-main}
:url-trip-metrics: {url-gh-root}/trip-metrics-app/{path-main}
:url-trip-consumer: {url-gh-root}/trip-consumer-app/{path-main}
:url-strimzi-doc-master: https://strimzi.io/docs/master
:url-amq-streams-doc-master: https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html/using_amq_streams_on_openshift_container_platform
:url-gh-strimzi-ko-master: https://github.com/strimzi/strimzi-kafka-operator/blob/master
:imagesdir: ../assets
:markup-in-source: verbatim,quotes
:sectanchors:
:sectlinks:


== Creating a Kafka Streams Application with AMQ Streams (Part 2)

Opening statement on the blog post series that can be included with all parts

. Basic
. Taxi
. Tracing

In this part of the blog post series we discuss a more in-depth example that builds upon the starting point from Part 1.

=== System Architecture

We are going to build up a solution that follows the architecture diagram below, so it may be worth referring back to this for each new component.

image::taxi-implementation.png[System Architecture,align="center"]

=== Dataset/Problem

The data we have chosen for this example is New York City taxi journey information from 2013, which was the dataset for the link:http://www.debs2015.org/call-grand-challenge.html[Distributed Event Based Systems (DEBS) Grand Challenge in 2015].
For a description of the source of the data, see https://chriswhong.com/open-data/foil_nyc_taxi/[the article here].

The dataset is provided as a CSV file, with the columns detailed below:

[caption=]
[cols="m,",options="header",%autowidth]
|===
|Column |Description
|medallion |an md5sum of the identifier of the taxi - vehicle bound
|hack_license |an md5sum of the identifier for the taxi license
|pickup_datetime |time when the passenger(s) were picked up
|dropoff_datetime |time when the passenger(s) were dropped off
|trip_time_in_secs |duration of the trip
|trip_distance |trip distance in miles
|pickup_longitude |longitude coordinate of the pickup location
|pickup_latitude |latitude coordinate of the pickup location
|dropoff_longitude |longitude coordinate of the drop-off location
|dropoff_latitude |latitude coordinate of the drop-off location
|payment_type |the payment method - credit card or cash
|fare_amount |fare amount in dollars
|surcharge |surcharge in dollars
|mta_tax |tax in dollars
|tip_amount |tip in dollars
|tolls_amount |bridge and tunnel tolls in dollars
|total_amount |total paid amount in dollars
|===
.source: DEBS 2015 Grand Challenge

{empty} +
There are several different interesting avenues that could be explored within this dataset, for example:

* We could follow specific taxis to calculate the takings from one taxi throughout the course of a day, or calculate the distance from their last drop off to the next pickup to find out whether they are travelling far without a passenger
* By using the distance and time of the taxi trip we could calculate the average speed, and use the coordinates of the pickup and drop off to try to guess the amount of traffic encountered

The processing we have chosen for this example is relatively straightforward - we calculate the total amount of money (`fare_amount + tip_amount`) earned within a particular area of the city, based off of journeys starting there.
This involves splitting the input data into a grid of different cells, and summing the total amount of money taken for every journey that originates from any cell.
To do this we have to consider splitting up processing in a way that ensures correctness of our output.

We will build this example up step-by-step, building upon what we learned in Part 1 of this blog post series.


=== Getting Data into Kafka

First things first, we need to make our dataset accessible from the cluster.
This would normally involve connecting to some service where we can poll the live data in real-time, but the data we have chosen is historical, and so we have to emulate the real-time behaviour.

Kafka Connector's are ideal for this sort of function, and are exactly what we are going to use.
However, we will avoid all the additional complexity of Kafka Connect for the time being, and use a KafkaProducer application in the same way as discussed in Part 1 of this blog post series.
This is done by simply including the (smaller) data file in the JAR, and reading line-by-line to send to Kafka.
See link:{url-taxi-producer}/TaxiProducerExample.java[TaxiProducerExample] for an example of how this works.
We set the `TOPIC` to write the data to as `taxi-source-topic` in the deployment configuration.

Lets check that the data is streaming to the topic:

[source,bash,options="nowrap",subs="{markup-in-source},replacements"]
----
$ oc run kafka-consumer -ti --image=registry.access.redhat.com/amq7/amq-streams-kafka:1.1.0-kafka-2.1.1 --rm=true --restart=Never \-- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic taxi-source-topic --from-beginning

_07290D3599E7A0D62097A346EFCC1FB5,E7750A37CAB07D0DFF0AF7E3573AC141,2013-01-01 00:00:00,2013-01-01 00:02:00,120,0.44,-73.956528,40.716976,-73.962440,40.715008,CSH,3.50,0.50,0.50,0.00,0.00,4.50
22D70BF00EEB0ADC83BA8177BB861991,3FF2709163DE7036FCAA4E5A3324E4BF,2013-01-01 00:02:00,2013-01-01 00:02:00,0,0.00,0.000000,0.000000,0.000000,0.000000,CSH,27.00,0.00,0.50,0.00,0.00,27.50
0EC22AAF491A8BD91F279350C2B010FD,778C92B26AE78A9EBDF96B49C67E4007,2013-01-01 00:01:00,2013-01-01 00:03:00,120,0.71,-73.973145,40.752827,-73.965897,40.760445,CSH,4.00,0.50,0.50,0.00,0.00,5.00
\..._
----


=== Kafka Streams Operations on the Data

Now that we have a topic with the `String` data we can start creating our application logic.
First up lets set up the configuration options for the Kafka streams application.
We do this in a separate config class, see link:{url-trip-convert}/TripConvertConfig.java[TripConvertConfig], which still uses the same method of reading from environment variables, as described in Part 1 of this blog post series.
It should be noted that this same method of providing configuration is used for each new application we build.

We can now generate the configuration options:

[source,java,options="nowrap"]
----
TripConvertConfig config = TripConvertConfig.fromMap(System.getenv());
Properties props = TripConvertConfig.createProperties(config);
----

As described in the basic example, the actions we perform will be to read from one topic, perform some kind of operation on the data, and write out to another topic.

Lets create the source stream in the same method as we have seen before:

[source,java,options="nowrap"]
----
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> source = builder.stream(config.getSourceTopic());
----

The data we receive is currently in a format that is not easy to use.
Our long CSV data is represented as a `String`, and we do not have access to the individual fields.
To perform operations on the data, we need to convert the `<String, String>` (`<key, value>`) events into the type that we know of.
For this, we have created a POJO representing the `Trip` data type, and an `enum TripFields` representing the columns of each data element.
The function `constructTripFromString` takes each of the lines of CSV data, and converts them into ``Trip``s.
This is implemented in the link:{url-trip-convert}/TripConvertApp.java[TripConvertApp] class.

The Kafka Streams DSL makes it easy to perform this function for every new record we receive:

[source,java,options="nowrap"]
----
KStream<String, Trip> mapped = source
                .map((key, value) -> {
                    new KeyValue<>(key, constructTripFromString(value))
                });
----

We could now write the `mapped` stream out to the sink topic, however the serialisation/deserialisation (SerDes) process for our value field has changed from the `Serdes.String()` that we set it to from the link:{url-trip-convert}/TripConvertConfig.java[TripConvertConfig] class.
As our `Trip` type is custom-built for our application, we must create our own SerDes implementation.
This is where the link:{url-trip-convert}/json/JsonObjectSerde.java[JsonObjectSerde] class comes into play.
We are using the https://vertx.io/docs/apidocs/io/vertx/core/json/JsonObject.html[Vertx JsonObject] implementation, and including our class type in the constructor to save us doing the hard work, although a different implementation may be better suited to another application.
The original `Trip` type only needs adjusting with appropriate `@JsonCreator` and `@JsonProperty` annotations.

We are now ready to output to our the `sinkTopic`, using the following command:

[source,java,options="nowrap"]
----
final JsonObjectSerde<Trip> tripSerde = new JsonObjectSerde<>(Trip.class);
mapped.to(config.getSinkTopic(), Produced.with(Serdes.String(), tripSerde));
----

==== Application Specific Information

The intention for our application is to calculate the total monies received by all journeys originating from any particular cell.
We therefore must perform some calculations using the journey origin latitude and longitude, to determine the cell it belongs to.
We use the logic laid out in the DEBS Grand Challenge for defining the specifics of the grid.
See the figure below for an example.

image::taxi-grid.png[Taxi Grid Example,align="center"]

We must set the origin of the grid (blue point), which represents the centre of grid cell (1,1), and a size in metres for every cell in the grid.
The cell size is converted into a latitude and longitude distance, `dy` and `dx` respectively, and the position of the top left of the grid is calculated (red point).
For any new arrival point we can easily count how many `dy` and `dx` away the coordinates are, and therefore in the example above (yellow point), we can determine that the journey originates from cell (3,4).

The additional application logic in the link:{url-trip-convert}/trip/Cell.java[Cell] class and link:{url-trip-convert}/TripConvertApp.java[TripConvertApp] perform this calculation, and we set the key of the new records as the `Cell` type.
To write to the `sinkTopic` we need a new SerDes, created in an identical fashion to the one we made before.

As we are using the default partitioning strategy, records are partitioned based on the different values of the keys, and so this change ensures that every `Trip` corresponding to a particular pickup `Cell` are distributed to the same partition.
When we perform processing downstream, the same processing node will receive all records corresponding to the same pickup cell, ensuring correctness and reproducibility of the operations.

=== Aggregation

We now have converted all of the incoming data to a type of `<Cell, Trip>`, and we would like to perform an aggregation operation.
Our intention is to calculate the sum of the `fare_amount + tip_amount` for every journey originating from one pickup cell, across a set time period.

As our data is historical, the time window that we use should be in relation to the original time that the events occurred, rather than the time that the event entered the Kafka system.
To do this, we need to provide a method of extracting this information from each record - a class that implements `TimestampExtractor`.
The `Trip` fields already contain this information for pickup and drop off times, and so the implementation is straightforward - see the implementation in link:{url-trip-metrics}/trip/TripTimestampExtractor.java[TripTimestampExtractor] for details.

Even though the topic we read from is already partitioned by cell, there are many more cells than partitions, and so each of our replicas will process the data for more than one cell.
To ensure that the windowing and aggregation is performed on a cell-by-cell basis, the `groupByKey()` function is called first, followed by a subsequent windowing operation.
As can be seen below, the window size is easily changeable, although for the time being we have opted for a window of 15 minutes.
The data can now be aggregated to generate the output metric we would like.
This is as simple as providing an accumulator value and the operation to perform for each record.
The output is of type `KTable`, where each key represents one particular window, and the value is the output of our aggregation opereation.
We use the `toStream()` function to convert it back to a kafka stream so that it can be output to the sink profit.

[source,java,options="nowrap"]
----
KStream<Cell, Trip> source = builder.stream(config.getSourceTopic(), Consumed.with(cellSerde, tripSerde));
KStream<Windowed<Cell>, Double> windowed = source
        .groupByKey(Serialized.with(cellSerde, tripSerde))
        .windowedBy(TimeWindows.of(TimeUnit.MINUTES.toMillis(15)))
        .aggregate(
                () -> (double) 0,
                (key, value, profit) -> {
                    profit + value.getFareAmount() + value.getTipAmount()
                },
                Materialized.<Cell, Double, WindowStore<Bytes, byte[]>>as("profit-store")
                        .withValueSerde(Serdes.Double()))
        .toStream();
----

As we do not require the information of which window the values belong to, we re-set the cell as the records keys, and round the value to two decimal places.

[source,java,options="nowrap"]
----
KStream<Cell, Double> rounded = windowed
                .map((window, profit) -> new KeyValue<>(window.key(), (double) Math.round(profit*100)/100));
----

Finally, the data can now be written to the output topic using the same method as defined before.

[source,java,options="nowrap"]
----
rounded.to(config.getSinkTopic(), Produced.with(cellSerde, Serdes.Double()));
----

=== Consume and Visualise

We now have the windowed cell-based metric being output to the last topic, so the final step is to consume and visualise the data.
For this, we use the link:https://vertx.io/docs/vertx-kafka-client/java/[Vertx Kafka Client] to read the data from our topic, and stream it to a JavaScript dashboard using the link:https://vertx.io/docs/vertx-core/java/#event_bus[Vertx EventBus] and link:https://github.com/sockjs[SockJS] (WebSockets). See link:{url-trip-consumer}/TripConsumerApp.java[TripConsumerApp] for the implementation.

This consumer application registers a handler that converts arriving records into a readable JSON format, and publishes it over an outbound EventBus channel.
The JavaScript connects to this channel and registers a handler for all incoming messages that performs relevant actions to visualise the data.

[source,java,options="nowrap"]
----
KafkaConsumer<String, Double> consumer = KafkaConsumer.create(vertx, props, String.class, Double.class);
consumer.handler(record -> {
    JsonObject json = new JsonObject();
    json.put("key", record.key());
    json.put("value", record.value());
    vertx.eventBus().publish("dashboard", json);
});
----

We log the information in a window so that the raw metric information can be seen, and use a geographical mapping library (https://leafletjs.com/[Leaflet]) to draw the original cells, modifying the opacity based on the value of the metric.

image::dashboard.png[Screenshot of Dashboard,align="center"]

By modifying the starting latitude and longitude, or the cell size (in both link:../trip-consumer-app/src/main/resources/webroot/index.html[index.html] and link:{url-trip-convert}/TripConvertApp.java[TripConvertApp]) you can change the grid that is being worked with.
You can also adjust the logic in the aggregate function to calculate some alternative metric from the data.

=== Creating a Kafka Connector

Up until now the producer we are using has been sufficient, even though the JAR (and image) are more bloated due to the additional data file.
However if we wanted to process the full 12GB dataset, this would not be an ideal solution.

The example connector we have built relies on hosting the file on an FTP server, but it should be noted that there are existing connectors for several different file stores.
We picked an FTP server as it allows our connector to easily communicate with a file external to the cluster.
For convenience, we use a python library `pyftpdlib` to host the file with the username and password set to `amqstreams`. However, hosting the file on any publicly accessible FTP server is sufficient.

A Kafka Connector consists of both itself, and tasks (also known as workers) that perform the retrieval of the data through calls to the `poll()` function.
The connector passes configuration over to the workers, and several workers can be invoked as per the `tasks.max` parameter.
We have created an link:{url-taxi-connect}/util/FTPConnection.java[FTPConnection] class, which provides the functions we require from the Apache Commons link:https://commons.apache.org/proper/commons-net/apidocs/org/apache/commons/net/ftp/FTPClient.html[FTPClient].
On each call to `poll()` we retrieve the next line from the file, and publish this record to the topic provided in the configuration.

We need to add our connector plugin to the existing `amq-streams-kafka-connect` docker image, which is done by adding the JAR to the plugins folder, as described in the link:{url-amq-streams-doc-master}/getting-started-str#using-kafka-connect-with-plug-ins-str[AMQ Streams documentation].
We can then deploy the Kafka Connect cluster, using the instructions from the link:{url-amq-streams-doc-master}/getting-started-str#deploying-kafka-connect-openshift-str[default KafkaConnect example], but adding the `spec.image` field to our `kafka-connect.yaml`, pointing to the image containing our plugin.

KafkaConnect is exposed as a RESTful resource, and so to check which connector plugins are present we can run the following `GET` request:

[source,bash,options="nowrap",subs="{markup-in-source}"]
----
$ oc exec -c kafka -i my-cluster-kafka-0 -- curl -s -X GET \
    http://my-connect-cluster-connect-api:8083/connector-plugins

_[{"class":"io.strimzi.TaxiSourceConnector","type":"source","version":"1.0-SNAPSHOT"},{"class":"org.apache.kafka.connect.file.FileStreamSinkConnector","type":"sink","version":"2.1.0"},{"class":"org.apache.kafka.connect.file.FileStreamSourceConnector","type":"source","version":"2.1.0"}]_

----

Similarly, to create a new connector we can `POST` the `JSON` configuration, as shown in the example below.
This new connector instance will establish an FTP connection to the server, and stream the data to the `taxi-source-topic`.
For this to work correctly, the following configuration options must be set correctly.

* `connect.ftp.address` - FTP connection URL host:port.

* `connect.ftp.filepath` - Path to file on remote FTP server (from root).

Optional configuration:

* `connect.ftp.attempts` - Maximum number of attempts to retrieve a valid FTP connection. (default: 3)

* `connect.ftp.backoff.ms` - Backoff time in milliseconds between connection attempts. (default: 10000ms)

[source,bash,options="nowrap",subs="{markup-in-source}"]
----
$ oc exec -c kafka -i my-cluster-kafka-0 -- curl -s -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://my-connect-cluster-connect-api:8083/connectors -d @- <<'EOF'

{
    "name": "taxi-connector",
    "config": {
        "connector.class": "io.strimzi.TaxiSourceConnector",
        "connect.ftp.address": "<ip-address>",
        "connect.ftp.user": "amqstreams",
        "connect.ftp.password": "amqstreams",
        "connect.ftp.filepath": "sorteddata.csv",
        "connect.ftp.topic": "taxi-source-topic",
        "tasks.max": "1",
        "value.converter": "org.apache.kafka.connect.storage.StringConverter"
    }
}
EOF

_{"name":"taxi-connector","config":{"connector.class":"io.strimzi.TaxiSourceConnector","connect.ftp.address":"10.56.222.49","connect.ftp.user":"amqstreams","connect.ftp.password":"amqstreams","connect.ftp.filepath":"sorteddata.csv","connect.ftp.topic":"taxi-source-topic","tasks.max":"1","value.converter":"org.apache.kafka.connect.storage.StringConverter","name":"taxi-connector"},"tasks":[],"type":null}_
----

We can `GET` the current deployed connectors:

[source,bash,options="nowrap",subs="{markup-in-source}"]
----
$ oc exec -c kafka -i my-cluster-kafka-0 -- curl -s -X GET \
    http://my-connect-cluster-connect-api:8083/connectors

_["taxi-connector"]_
----

We can check if the data is streaming to the topic in the same way as we did in section link:#_getting_data_into_kafka[Getting Data into Kafka].
For debugging information, see the logs of `my-connect-cluster-connect`.
To stop the plugin we can delete it with the following command.

[source,bash,options="nowrap"]
----
$ oc exec -c kafka -i my-cluster-kafka-0 -- curl -s -X DELETE \
    http://my-connect-cluster-connect-api:8083/connectors/taxi-connector
----

That's it. We have managed to create a more complex streams application, where we source (historical) real-world data with a Kafka Connector, stream it through multiple processing points, and sink it to a visualisation.
