:path-main: src/main/java/io/strimzi
:url-taxi-connect: ../taxi-connect/{path-main}
:url-trip-convert: ../trip-convert-app/{path-main}
:url-trip-metrics: ../trip-metrics-app/{path-main}
:url-trip-consumer: ../trip-consumer-app/{path-main}
:url-entrypoint: https://github.com/adam-cattermole/strimzi-lab/tree/add-taxi-example/taxi-example
:url-strimzi-doc-master: https://strimzi.io/docs/master
:url-gh-strimzi-ko-master: https://github.com/strimzi/strimzi-kafka-operator/blob/master

=== Dataset/Problem

The data we have chosen for this example is New York City taxi journey information from 2013, which was the dataset for the link:http://www.debs2015.org/call-grand-challenge.html[Distributed Event Based Systems (DEBS) Grand Challenge in 2015].
For a description of the source of the data, see https://chriswhong.com/open-data/foil_nyc_taxi/[the article here].

The dataset is provided as a CSV file, with the columns detailed below:

[caption=]
[cols="m,",options="header",%autowidth]
|===
|Column |Description
|medallion |an md5sum of the identifier of the taxi - vehicle bound
|hack_license |an md5sum of the identifier for the taxi license
|pickup_datetime |time when the passenger(s) were picked up
|dropoff_datetime |time when the passenger(s) were dropped off
|trip_time_in_secs |duration of the trip
|trip_distance |trip distance in miles
|pickup_longitude |longitude coordinate of the pickup location
|pickup_latitude |latitude coordinate of the pickup location
|dropoff_longitude |longitude coordinate of the drop-off location
|dropoff_latitude |latitude coordinate of the drop-off location
|payment_type |the payment method - credit card or cash
|fare_amount |fare amount in dollars
|surcharge |surcharge in dollars
|mta_tax |tax in dollars
|tip_amount |tip in dollars
|tolls_amount |bridge and tunnel tolls in dollars
|total_amount |total paid amount in dollars
|===
.source: DEBS 2015 Grand Challenge

{empty} +
There are several different interesting avenues that could be explored within this dataset, for example:

* We could follow specific taxis to calculate the takings from one taxi throughout the course of a day, or calculate the distance from their last drop off to the next pickup to find out whether they are travelling far without a passenger
* By using the distance and time of the taxi trip we could calculate the average speed, and use the coordinates of the pickup and drop off to try to guess the amount of traffic encountered

The processing we have chosen for this example is relatively straightforward - we calculate the total amount of money (`fare_amount + tip_amount`) earned within a particular area of the city, based off of journeys starting there.
This involves splitting the input data into a grid of different cells, and summing the total amount of money taken for every journey that originates from any cell.
To do this we have to consider splitting up processing in a way that ensures correctness of our output.

We will build this example up step-by-step, starting with KafkaConnect and a custom built connector.


=== Getting Data into the System

First things first, we need to make our dataset accessible from the cluster.
This would normally involve connecting to some service where we can poll the live data in real-time, but the data we have chosen is historical, and so we have to simulate the real-time behaviour.

The example connector we have built relies on hosting the file on an FTP server.
We picked this method as it allows our connector to easily communicate with a file external to the cluster.
For convenience, we use a python library `pyftpdlib` to host the file with the username and password set to `strimzi`.
For instructions on running the FTP server see link:{url-entrypoint}#running-the-python-ftp-server[Running the Python FTP server].

Before we continue, lets check that we can connect from the cluster to the FTP server running on our host, by deploying a simple container. You should see something similar to the output below

[source,bash,options="nowrap",subs="{markup-in-source},replacements"]
----
$ kubectl run test-ftp -ti --generator=run-pod/v1 --image=centos:latest --rm=true -n kafka \-- /bin/bash

_If you don\'t see a command prompt, try pressing enter._

$ yum install -y ftp

_\...
Installed:
  ftp.x86_64 0:0.17-67.el7

Complete!_

$ ftp -pvn <ip-address>

_Connected to <ip-address> (<ip-address>).
220 pyftpdlib 1.5.5 ready.
Remote system type is UNIX.
Using binary mode to transfer files._

$ ftp> user strimzi strimzi

_331 Username ok, send password.
230 Login successful._
----

A Kafka Connector consists of both itself, and tasks (or workers) that perform the retrieval of the data through the `poll()` function.
The connector passes configuration over to the workers, and several workers can be invoked as per the `tasks.max` parameter.
We have created an link:{url-taxi-connect}/util/FTPConnection.java[FTPConnection] class, which provides the functions we require from the Apache Commons link:https://commons.apache.org/proper/commons-net/apidocs/org/apache/commons/net/ftp/FTPClient.html[FTPClient].
On each call to `poll()` we retrieve the next line from the file, and publish this record to the topic provided in the configuration.

We need to add our connector plugin to the existing link:https://hub.docker.com/r/strimzi/kafka-connect[strimzi/kafka-connect] docker image, which is done by adding the JAR to the plugins folder, as described in the link:{url-strimzi-doc-master}/#creating-new-image-from-base-str[Strimzi documentation].
We can then deploy the Kafka Connect cluster, using the configuration from the link:{url-gh-strimzi-ko-master}/examples/kafka-connect/kafka-connect.yaml[existing KafkaConnect example], but adding the `spec.image` field to point to the image containing our plugin.

KafkaConnect is exposed as a RESTful resource, and so to check which connector plugins are present we can run the following `GET` request:

[source,bash,options="nowrap",subs="{markup-in-source}"]
----
$ kubectl exec -c kafka -i my-cluster-kafka-0 -n kafka -- curl -s -X GET \
    http://my-connect-cluster-connect-api:8083/connector-plugins

_[{"class":"io.strimzi.TaxiSourceConnector","type":"source","version":"1.0-SNAPSHOT"},{"class":"org.apache.kafka.connect.file.FileStreamSinkConnector","type":"sink","version":"2.1.0"},{"class":"org.apache.kafka.connect.file.FileStreamSourceConnector","type":"source","version":"2.1.0"}]_

----

Similarly, to create a new connector we can `POST` the `JSON` configuration, as shown in the example below.
This new connector instance will establish an FTP connection to the server, and stream the data to the `taxi-source-topic`.
For this to work correctly, the following configuration options must be set correctly.

* `connect.ftp.address` - FTP connection URL host:port.

* `connect.ftp.filepath` - Path to file on remote FTP server (from root).

Optional configuration:

* `connect.ftp.attempts` - Maximum number of attempts to retrieve a valid FTP connection. (default: 3)

* `connect.ftp.backoff.ms` - Backoff time in milliseconds between connection attempts. (default: 10000ms)

[source,bash,options="nowrap",subs="{markup-in-source}"]
----
$ kubectl exec -c kafka -i my-cluster-kafka-0 -n kafka -- curl -s -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://my-connect-cluster-connect-api:8083/connectors -d @- <<'EOF'

{
    "name": "taxi-connector",
    "config": {
        "connector.class": "io.strimzi.TaxiSourceConnector",
        "connect.ftp.address": "<ip-address>",
        "connect.ftp.user": "strimzi",
        "connect.ftp.password": "strimzi",
        "connect.ftp.filepath": "sorteddata.csv",
        "connect.ftp.topic": "taxi-source-topic",
        "tasks.max": "1",
        "value.converter": "org.apache.kafka.connect.storage.StringConverter"
    }
}
EOF

_{"name":"taxi-connector","config":{"connector.class":"io.strimzi.TaxiSourceConnector","connect.ftp.address":"10.56.222.49","connect.ftp.user":"strimzi","connect.ftp.password":"strimzi","connect.ftp.filepath":"sorteddata.csv","connect.ftp.topic":"taxi-source-topic","tasks.max":"1","value.converter":"org.apache.kafka.connect.storage.StringConverter","name":"taxi-connector"},"tasks":[],"type":null}_
----

We can `GET` the current deployed connectors:

[source,bash,options="nowrap",subs="{markup-in-source}"]
----
$ kubectl exec -c kafka -i my-cluster-kafka-0 -n kafka -- curl -s -X GET \
    http://my-connect-cluster-connect-api:8083/connectors

_["taxi-connector"]_
----

Lets check that the data is streaming to the topic:

[source,bash,options="nowrap",subs="{markup-in-source},replacements"]
----
$ kubectl run kafka-consumer -ti --image=strimzi/kafka:0.11.1-kafka-2.1.0 --rm=true --restart=Never -n kafka \-- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic taxi-source-topic --from-beginning

_If you don\'t see a command prompt, try pressing enter.
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
07290D3599E7A0D62097A346EFCC1FB5,E7750A37CAB07D0DFF0AF7E3573AC141,2013-01-01 00:00:00,2013-01-01 00:02:00,120,0.44,-73.956528,40.716976,-73.962440,40.715008,CSH,3.50,0.50,0.50,0.00,0.00,4.50
22D70BF00EEB0ADC83BA8177BB861991,3FF2709163DE7036FCAA4E5A3324E4BF,2013-01-01 00:02:00,2013-01-01 00:02:00,0,0.00,0.000000,0.000000,0.000000,0.000000,CSH,27.00,0.00,0.50,0.00,0.00,27.50
0EC22AAF491A8BD91F279350C2B010FD,778C92B26AE78A9EBDF96B49C67E4007,2013-01-01 00:01:00,2013-01-01 00:03:00,120,0.71,-73.973145,40.752827,-73.965897,40.760445,CSH,4.00,0.50,0.50,0.00,0.00,5.00
\..._
----

For debugging information, see the logs of `my-connect-cluster-connect`.

While we work on the rest of the application we can stop the plugin:

[source,bash,options="nowrap"]
----
$ kubectl exec -c kafka -i my-cluster-kafka-0 -n kafka -- curl -s -X DELETE \
    http://my-connect-cluster-connect-api:8083/connectors/taxi-connector
----

=== Kafka Streams Operations on the Data

Now that we have a topic with the `String` data we can start creating our application logic.
First up lets set up the configuration options for the Kafka streams application.
We do this in a separate config class, see link:{url-trip-convert}/TripConvertConfig.java[TripConvertConfig], which still uses the same method of reading from environment variables, as described in the initial example.
It should be noted that this same method of providing configuration is used for each new application we build.

We can now generate the configuration options:

[source,java,options="nowrap"]
----
TripConvertConfig config = TripConvertConfig.fromMap(System.getenv());
Properties props = TripConvertConfig.createProperties(config);
----

As described in the basic example, the actions we perform will be to read from one topic, perform some kind of operation on the data, and write out to another topic.

Lets create the source stream in the same method as we have seen before:

[source,java,options="nowrap"]
----
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> source = builder.stream(config.getSourceTopic());
----

The data we receive is currently in a format that is not easy to use.
Our long CSV data is represented as a `String`, and we do not have access to the individual fields.
To perform operations on the data, we need to convert the `<String, String>` (`<key, value>`) events into the type that we know of.
For this, we have created a POJO representing the `Trip` data type, and an `enum TripFields` representing the columns of each data element.
The function `constructTripFromString` takes each of the lines of CSV data, and converts them into ``Trip``s.
This is implemented in the link:{url-trip-convert}/TripConvertApp.java[TripConvertApp] class.

The Kafka Streams DSL makes it easy to perform this function for every new record we receive:

[source,java,options="nowrap"]
----
KStream<String, Trip> mapped = source
                .map((key, value) -> {
                    new KeyValue<>(key, constructTripFromString(value))
                });
----

We could now write the `mapped` stream out to the sink topic, however the serialisation/deserialisation (SerDes) process for our value field has changed from the `Serdes.String()` that we set it to from the link:{url-trip-convert}/TripConvertConfig.java[TripConvertConfig] class.
As our `Trip` type is custom-built for our application, we must create our own SerDes implementation.
This is where the link:{url-trip-convert}/json/JsonObjectSerde.java[JsonObjectSerde] class comes into play.
We are using the https://vertx.io/docs/apidocs/io/vertx/core/json/JsonObject.html[Vertx JsonObject] implementation, and including our class type in the constructor to save us doing the hard work, although a different implementation may be better suited to another application.
The original `Trip` type only needs adjusting with appropriate `@JsonCreator` and `@JsonProperty` annotations.

We are now ready to output to our the `sinkTopic`, using the following command:

[source,java,options="nowrap"]
----
final JsonObjectSerde<Trip> tripSerde = new JsonObjectSerde<>(Trip.class);
mapped.to(config.getSinkTopic(), Produced.with(Serdes.String(), tripSerde));
----

==== Application Specific Information

The intention for our application is to calculate the total monies received by all journeys originating from any particular cell.
We therefore must perform some calculations using the journey origin latitude and longitude, to determine the cell it belongs to.
We use the logic laid out in the DEBS Grand Challenge for defining the specifics of the grid.
See the figure below for an example.

image::taxi-grid.png[Taxi Grid Example,align="center"]

We must set the origin of the grid (blue point), which represents the centre of grid cell (1,1), and a size in metres for every cell in the grid.
The cell size is converted into a latitude and longitude distance, `dy` and `dx` respectively, and the position of the top left of the grid is calculated (red point).
For any new arrival point we can easily count how many `dy` and `dx` away the coordinates are, and therefore in the example above (yellow point), we can determine that the journey originates from cell (3,4).

The additional application logic in the link:{url-trip-convert}/trip/Cell.java[Cell] class and link:{url-trip-convert}/TripConvertApp.java[TripConvertApp] perform this calculation, and we set the key of the new records as the `Cell` type.
To write to the `sinkTopic` we need a new SerDes, created in an identical fashion to the one we made before.

As we are using the default partitioning strategy, records are partitioned based on the different values of the keys, and so this change ensures that every `Trip` corresponding to a particular pickup `Cell` are distributed to the same partition.
When we perform processing downstream, the same processing node will receive all records corresponding to the same pickup cell, ensuring correctness and reproducibility of the operations.

=== Aggregation

We now have converted all of the incoming data to a type of `<Cell, Trip>`, and we would like to perform an aggregation operation.
Our intention is to calculate the sum of the `fare_amount + tip_amount` for every journey originating from one pickup cell, across a set time period.

As our data is historical, the time window that we use should be in relation to the original time that the events occurred, rather than the time that the event entered the Kafka system.
To do this, we need to provide a method of extracting this information from each record - a class that implements `TimestampExtractor`.
The `Trip` fields already contain this information for pickup and drop off times, and so the implementation is straightforward - see the implementation in link:{url-trip-metrics}/trip/TripTimestampExtractor.java[TripTimestampExtractor] for details.

Even though the topic we read from is already partitioned by cell, there are many more cells than partitions, and so each of our replicas will process the data for more than one cell.
To ensure that the windowing and aggregation is performed on a cell-by-cell basis, the `groupByKey()` function is called first, followed by a subsequent windowing operation.
As can be seen below, the window size is easily changeable, although for the time being we have opted for a window of 15 minutes.
The data can now be aggregated to generate the output metric we would like.
This is as simple as providing an accumulator value and the operation to perform for each record.
The output is of type `KTable`, where each key represents one particular window, and the value is the output of our aggregation opereation.
We use the `toStream()` function to convert it back to a kafka stream so that it can be output to the sink profit.

[source,java,options="nowrap"]
----
KStream<Cell, Trip> source = builder.stream(config.getSourceTopic(), Consumed.with(cellSerde, tripSerde));
KStream<Windowed<Cell>, Double> windowed = source
        .groupByKey(Serialized.with(cellSerde, tripSerde))
        .windowedBy(TimeWindows.of(TimeUnit.MINUTES.toMillis(15)))
        .aggregate(
                () -> (double) 0,
                (key, value, profit) -> {
                    profit + value.getFareAmount() + value.getTipAmount()
                },
                Materialized.<Cell, Double, WindowStore<Bytes, byte[]>>as("profit-store")
                        .withValueSerde(Serdes.Double()))
        .toStream();
----

As we do not require the information of which window the values belong to, we re-set the cell as the records keys, and round the value to two decimal places.

[source,java,options="nowrap"]
----
KStream<Cell, Double> rounded = windowed
                .map((window, profit) -> new KeyValue<>(window.key(), (double) Math.round(profit*100)/100));
----

Finally, the data can now be written to the output topic using the same method as defined before.

[source,java,options="nowrap"]
----
rounded.to(config.getSinkTopic(), Produced.with(cellSerde, Serdes.Double()));
----

=== Consume and Visualise

We now have the windowed cell-based metric being output to the last topic, so the final step is to consume and visualise the data.
For this, we use the link:https://vertx.io/docs/vertx-kafka-client/java/[Vertx Kafka Client] to read the data from our topic, and stream it to a JavaScript dashboard using the link:https://vertx.io/docs/vertx-core/java/#event_bus[Vertx EventBus] and link:https://github.com/sockjs[SockJS] (WebSockets). See link:{url-trip-consumer}/TripConsumerApp.java[TripConsumerApp] for the implementation.

This consumer application registers a handler that converts arriving records into a readable JSON format, and publishes it over an outbound EventBus channel.
The JavaScript connects to this channel and registers a handler for all incoming messages that performs relevant actions to visualise the data.

[source,java,options="nowrap"]
----
KafkaConsumer<String, Double> consumer = KafkaConsumer.create(vertx, props, String.class, Double.class);
consumer.handler(record -> {
    JsonObject json = new JsonObject();
    json.put("key", record.key());
    json.put("value", record.value());
    vertx.eventBus().publish("dashboard", json);
});
----

We log the information in a window so that the raw metric information can be seen, and use a geographical mapping library (https://leafletjs.com/[Leaflet]) to draw the original cells, modifying the opacity based on the value of the metric.

image::dashboard.png[Screenshot of Dashboard,align="center"]

By modifying the starting latitude and longitude, or the cell size (in both link:../trip-consumer-app/src/main/resources/webroot/index.html[index.html] and link:{url-trip-convert}/TripConvertApp.java[TripConvertApp]) you can change the grid that is being worked with.
You can also adjust the logic in the aggregate function to calculate some alternative metric from the data.

=== System Architecture

The application that we have built is detailed in the architecture diagram below.
Each of the topics are labelled with the keys and values that they contain.

image::taxi-implementation.png[System Architecture,align="center"]
